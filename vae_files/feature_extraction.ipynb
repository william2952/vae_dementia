{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction code",
   "id": "4e447219d60d5d11"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T15:02:43.422688Z",
     "start_time": "2024-08-20T15:02:25.282670Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nilearn\n",
    "from nilearn import plotting, image, surface, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nibabel\n",
    "import ants\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "print(torch.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")",
   "id": "c0e69d20e906047f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Input de-identified, non-labeled training, testing datasets",
   "id": "4c55602cbaf38142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_csv = pd.read_csv('model_data/test_brain_labels_deid.csv', index_col=0)\n",
    "test_parquet_scaled = pd.read_parquet('model_data/test_brain_data_deid_scaled.parquet')\n",
    "\n",
    "train_csv = pd.read_csv('model_data/train_brain_labels_deid.csv', index_col=0)\n",
    "train_parquet_scaled = pd.read_parquet('model_data/train_brain_data_deid_scaled.parquet')\n",
    "nd_csv = pd.read_csv('model_data/nd_brain_labels_deid.csv', index_col=0)\n",
    "nd_parquet_scaled = pd.read_parquet('model_data/nd_brain_data_deid_scaled.parquet')"
   ],
   "id": "14dd3c749a0b4d25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load brain mask and prepare it",
   "id": "ca347c156936f35f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain_mask = nilearn.image.load_img('model_data/brain_mask.nii')\n",
    "img_data = np.zeros(brain_mask.shape)\n",
    "vec = train_parquet_scaled.iloc[0, 0]\n",
    "nz_indices = np.ma.nonzero(brain_mask.get_fdata())"
   ],
   "id": "ea9df941a9da28c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 16\n",
    "k_normalized = torch.load('model_data/tensor_k_norm.pt')"
   ],
   "id": "c48583c3b2a1b01c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Custom dataset",
   "id": "d2ad62376db3dc17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomLoader(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        super(CustomLoader, self).__init__()\n",
    "        \n",
    "        self.images = images # parquet\n",
    "        self.transform = transform\n",
    "\n",
    "        self.n_samples = self.images.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images.iloc[idx].copy()\n",
    "        image = np.array(image)\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.to(torch.float32)\n",
    "        image = image.unsqueeze(0)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ],
   "id": "efedd7370f58155d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load datasets using cutom dataset loader",
   "id": "dd13e47aefb7ed9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = CustomLoader(images=test_parquet_scaled, transform=None)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "nd_data = CustomLoader(images=nd_parquet_scaled, transform=None)\n",
    "nd_loader = DataLoader(dataset=nd_data, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)"
   ],
   "id": "cc9be3ad039f0155"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Graph convolution method",
   "id": "2163590449a02015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 264,
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, k):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.d1 = nn.Conv1d(in_dim, out_dim, kernel_size=1)\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        device = imgs.device\n",
    "        x = self.d1(imgs)\n",
    "        x = x.cpu() @ self.k.cpu()\n",
    "        x = x.to(device)\n",
    "        return x"
   ],
   "id": "aefe1e19aff76be5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Graph convolutional network block",
   "id": "f337ca3b7a3afd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 470,
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, k):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.k = k\n",
    "        self.GC1 = GraphConv(in_dim, out_dim, self.k)\n",
    "        self.GC2 = GraphConv(out_dim, out_dim, self.k)\n",
    "        self.bn1 = nn.BatchNorm1d(out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "    def forward(self, x):\n",
    "        identity = x.mean(axis=1, keepdim = True)\n",
    "        out = self.GC1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.tanh(out) #out = F.relu(out)\n",
    "        out = self.GC2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + identity\n",
    "        #out = F.relu(out)  #testing\n",
    "        return out"
   ],
   "id": "b7056b8f34a4399f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Encoder of the Variational autoencoder",
   "id": "9bf15ff7d2441aa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 471,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, conv_dim, lin_dim, latent_dims, k, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.k = k\n",
    "        self.device = device\n",
    "        self.GCBlock = GCNBlock(in_dim, conv_dim, self.k)\n",
    "\n",
    "        self.Lin1 = nn.Linear(conv_dim * self.k.shape[0], lin_dim)\n",
    "        self.Lin2 = nn.Linear(lin_dim, latent_dims)\n",
    "        self.Lin3 = nn.Linear(lin_dim, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        #self.N.loc = self.N.loc.to(device)\n",
    "        #self.N.scale = self.N.scale.to(device)\n",
    "        self.KL = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.GCBlock(x)\n",
    "        x = F.tanh(x) #x = F.relu(x)\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.Lin1(x)\n",
    "        x = F.tanh(x) #x = F.relu(x)\n",
    "\n",
    "        mu = self.Lin2(x)\n",
    "        sigma = torch.exp(self.Lin3(x))\n",
    "        z = mu + sigma * self.N.sample(mu.shape).to(self.device)\n",
    "        self.KL = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "\n",
    "        return z"
   ],
   "id": "db5e5f38dae7c80a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Decoder of the variational autoencoder",
   "id": "c4bc7b6ec8996c7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 472,
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, conv_dim, lin_dim,  latent_dims, k):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, lin_dim),\n",
    "            nn.Tanh(), #nn.relu(True),\n",
    "            nn.Linear(lin_dim, conv_dim * self.k.shape[0]), \n",
    "            nn.Tanh() #nn.relu(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(conv_dim, self.k.shape[0]))\n",
    "\n",
    "        self.decoder_conv = GCNBlock(conv_dim, in_dim, self.k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        \n",
    "        x = self.decoder_conv(x)\n",
    "        return x"
   ],
   "id": "4b5be8e4b20e5629"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Variational Autoencoder class",
   "id": "cc27559a80e241e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 473,
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim, conv_dim, lin_dim, latent_dims, k, device):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.encoder = Encoder(in_dim, conv_dim, lin_dim, latent_dims, self.k, self.device)\n",
    "        self.decoder = Decoder(in_dim, conv_dim, lin_dim, latent_dims, self.k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ],
   "id": "675dedc2fc204848"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Optimizer and variational autoencoder initialization and hyperparameters",
   "id": "7a230e7361e46927"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 474,
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "latent_dims = 16\n",
    "vae = VAE(1, 8, 256, latent_dims=latent_dims, k=k_normalized, device=device)\n",
    "\n",
    "vae.to(device)\n",
    "total_epochs = 0\n",
    "\n",
    "lr = 1e-5\n",
    "optim = torch.optim.Adam(vae.parameters(), lr=lr)"
   ],
   "id": "afe53ac6c0c8adf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load trained variational autoencoder model and optimizer",
   "id": "c93b457ea27df39a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vae.load_state_dict(torch.load('model_data/2gc_16dim_model_scaled.pt'))\n",
    "vae.eval()\n",
    "optim.load_state_dict(torch.load('model_data/2gc_16dim_optim_scaled.pt'))"
   ],
   "id": "cf77ec9aa7f2414a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Encode the test dataset",
   "id": "c367e2f68af474f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_encoded_samples = []\n",
    "for sample in tqdm(test_data):\n",
    "    img = sample.unsqueeze(0).to(device)\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded_img = vae.encoder(img)\n",
    "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
    "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
    "    test_encoded_samples.append(encoded_sample)\n",
    "test_encoded_samples = pd.DataFrame(test_encoded_samples)"
   ],
   "id": "198bbc6d05f0c5d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Decode the test dataset",
   "id": "74938e869e06a349"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_reconstructed_samples = []\n",
    "for sample in tqdm(test_data):\n",
    "    img = sample.unsqueeze(0).to(device)\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img= vae(img)\n",
    "    reconstructed_img = reconstructed_img.flatten().cpu().numpy()\n",
    "    reconstructed_sample = {f\"Rec. Pixel {i}\": pix for i, pix in enumerate(reconstructed_img)}\n",
    "    test_reconstructed_samples.append(reconstructed_sample)\n",
    "test_reconstructed_samples = pd.DataFrame(test_reconstructed_samples)"
   ],
   "id": "30388244262e835d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Decode the nd dataset",
   "id": "1fc95e37a6071260"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nd_reconstructed_samples = []\n",
    "for sample in tqdm(nd_data):\n",
    "    img = sample.unsqueeze(0).to(device)\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed_img= vae(img)\n",
    "    reconstructed_img = reconstructed_img.flatten().cpu().numpy()\n",
    "    reconstructed_sample = {f\"Rec. Pixel {i}\": pix for i, pix in enumerate(reconstructed_img)}\n",
    "    nd_reconstructed_samples.append(reconstructed_sample)\n",
    "nd_reconstructed_samples = pd.DataFrame(nd_reconstructed_samples)"
   ],
   "id": "3db081b6559afcdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_reconstructed_samples.to_parquet('model_data/recon_test_data.parquet')\n",
    "test_reconstructed = pd.read_parquet('model_data/recon_test_data.parquet')"
   ],
   "id": "60539876f3fab38b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Decode function",
   "id": "877ba0432e737a6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def decode_img(vae, out_img):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decoder(out_img).cpu().numpy()\n",
    "    decoded = decoded.reshape(1, -1) \n",
    "    return decoded"
   ],
   "id": "f742e2013acce24a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Show brain image function",
   "id": "ad8209143428c09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_single_img(vae, out_img, img_data, nz_indices, brain_mask, path, scaler):\n",
    "    decoded = decode_img(vae, out_img)\n",
    "    decoded = scaler.transform(decoded)\n",
    "    out_img_data = img_data\n",
    "    out_img_data[nz_indices] = decoded\n",
    "\n",
    "    out_img = nilearn.image.new_img_like(brain_mask, out_img_data)\n",
    "    nibabel.save(out_img, path)"
   ],
   "id": "43536d46cf627489"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Standard Scaler for averages",
   "id": "3fd4facaa428200b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scaler = StandardScaler().fit(test_reconstructed.values)\n",
    "std_data = torch.tensor(np.std(test_encoded_samples, axis=0))\n",
    "mean_embeddings = torch.tensor(np.mean(test_encoded_samples, axis=0))"
   ],
   "id": "9b2db57edc0ceaf3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Standard deviation array",
   "id": "7bf1dad440fa91ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "std_array = np.zeros((16, 5, 16))\n",
    "for dim in range(std_array.shape[0]):\n",
    "    for i in range(std_array.shape[1]):\n",
    "        std_array[dim, i] = mean_embeddings.squeeze()\n",
    "        std_array[dim, i, dim] += std_data[dim] * (i-2)\n",
    "std_array = torch.tensor(std_array, dtype=torch.float32)\n",
    "std_array = std_array.unsqueeze(2)"
   ],
   "id": "4714ca424b88612a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vae.to(device)\n",
    "std_array = std_array.to(device)"
   ],
   "id": "c1a91fd27fa50bfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create reconstructed manipulated images",
   "id": "10d06012661ae342"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for dim in range(std_array.shape[0]):\n",
    "    for i in [-2, -1, 0, 1, 2]:\n",
    "        path = f'path_files/dim_{dim}_i_{i}.nii.gz'\n",
    "        plot_single_img(vae, std_array[dim, i+2], img_data, nz_indices, brain_mask, path, scaler)"
   ],
   "id": "8e0f2bf33cc8b9f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bg = nilearn.image.load_img('model_data/mcalt_t1.nii')",
   "id": "4685611e80575815"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Show the feature extraction images",
   "id": "d45d89cf0bf3ab5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows=16, ncols=5, figsize=(15, 30))\n",
    "\n",
    "for dim in range(axes.shape[0]):\n",
    "    for i in range(axes.shape[1]):\n",
    "        if i == 2:\n",
    "            axes[dim, i].set_title(f\"Embedding Dim. {dim}\")\n",
    "        else:\n",
    "            axes[dim, i].set_title(f\"{i-2} std\")\n",
    "        path = f'path_files/dim_{dim}_i_{i-2}.nii.gz'\n",
    "        out_std_img = nilearn.image.load_img(path)\n",
    "        out_smooth = nilearn.image.smooth_img(out_std_img, fwhm=6)\n",
    "        nilearn.plotting.plot_stat_map(out_smooth, cut_coords=1, bg_img=bg, axes=axes[dim, i], colorbar=False, display_mode='x', vmax=2)\n",
    "plt.show()"
   ],
   "id": "974b167e029e7c29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Save std embeddings to csv",
   "id": "2c1484c90cc93373"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "std_array_out = std_array.cpu().squeeze(2).numpy()\n",
    "arr2d_std = np.empty((16, 5), dtype=object)\n",
    "for i in range(std_array_out.shape[0]):\n",
    "    for j in range(std_array_out.shape[1]):\n",
    "        arr2d_std[i, j] = std_array_out[i, j].tolist()\n",
    "std_df = pd.DataFrame(arr2d_std)\n",
    "std_df.to_csv('model_data/std_embedding_arr.csv')"
   ],
   "id": "517a2192692346ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Save histogram comparison of embeddings to csv",
   "id": "fc2a7e601dc2caaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vec = scaler.mean_\n",
    "dimg = decode_img(vae, mean_embeddings.to(device)).squeeze()\n",
    "hist_rec_comparison = pd.DataFrame((vec, dimg))\n",
    "hist_rec_comparison.to_csv('model_data/hist_rec_comparison.csv')"
   ],
   "id": "3abb83e7b35b1729"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### MCALT to MNI registration",
   "id": "684e7a13cc126488"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain_mask = nilearn.image.load_img('model_data/brain_mask.nii')\n",
    "mcalt = nilearn.image.load_img('model_data/mcalt_t1.nii')\n",
    "mni = nilearn.image.load_img('model_data/mni_t1.nii')\n",
    "ants_mcalt = ants.from_nibabel(mcalt)\n",
    "ants_mni = ants.from_nibabel(mni)\n",
    "reg_dict = ants.registration(fixed=ants_mni, moving=ants_mcalt, type_of_transform='SyN')\n",
    "ants_transform = reg_dict['fwdtransforms']"
   ],
   "id": "9ad5e6bcc9102e55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Surface rendering of features",
   "id": "aaa5a7a45b41a473"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_path_plot(path, hemi='right', view='lateral', mask=brain_mask, interactive=False, threshold=None, color_map='turbo', colorbar=True, transform=ants_transform, fixed=ants_mni, moving=ants_mcalt, vmin=None, vmax=None):\n",
    "    img = nilearn.image.load_img(path)\n",
    "    img_smooth = nilearn.image.smooth_img(img, fwhm=6)\n",
    "    ants_img = ants.from_nibabel(img_smooth)\n",
    "    ants_trans_img = ants.apply_transforms(fixed=fixed, moving=ants_img, transformlist=transform)\n",
    "    trans_img = ants.to_nibabel(ants_trans_img)\n",
    "    fsaverage = datasets.fetch_surf_fsaverage()\n",
    "    if hemi == 'right':\n",
    "        mesh = surface.load_surf_mesh(fsaverage.pial_right)\n",
    "    elif hemi == 'left':\n",
    "        mesh = surface.load_surf_mesh(fsaverage.pial_left)\n",
    "    texture = surface.vol_to_surf(trans_img, mesh)\n",
    "\n",
    "    \n",
    "    if not interactive:\n",
    "        fig = plotting.plot_surf_stat_map(mesh, texture, hemi=hemi, view=view, colorbar=colorbar, threshold=threshold, bg_map=fsaverage.sulc_right, cmap=color_map, vmin=vmin, vmax=vmax)                       \n",
    "    else:\n",
    "        fig = plotting.plot_surf_stat_map(mesh, texture, hemi=hemi, view=view, colorbar=colorbar, threshold=threshold, bg_map=fsaverage.sulc_right, cmap=color_map, vmin=vmin, vmax=vmax, engine='plotly') \n",
    "\n",
    "    return fig"
   ],
   "id": "553f370a9f95d6b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example figure code",
   "id": "916f909d37af3970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "figure = get_path_plot(path='path_files/dim_1_i_-2.nii.gz', view='lateral', colorbar=False)",
   "id": "3eb02b2e30acdd1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
