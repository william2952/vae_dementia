{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Variational Autoencoder code",
   "id": "d440a97c627c1ecd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T14:30:14.472892Z",
     "start_time": "2024-08-20T14:29:52.456072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nilearn.image\n",
    "from nilearn import plotting, image, surface, datasets\n",
    "import copy\n",
    "import ants\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "print(torch.__version__)"
   ],
   "id": "f152086405aff5e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Input Standard Scaled, de-identified, non-labeled training, testing datasets, and diagnosis-labeled test data\n",
    "\n",
    "SUVR FDG-PET voxel intensities can be accessed here:\n",
    "\n",
    "train_parquet = pd.read_parquet('model_data/train_brain_data_deid.parquet')\n",
    "\n",
    "test_parquet = pd.read_parquet('model_data/test_brain_data_deid.parquet')\n",
    "\n",
    "nd_parquet = pd.read_parquet('model_data/nd_brain_data_deid.parquet')"
   ],
   "id": "4288dd8281bb4572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_csv = pd.read_csv('model_data/train_brain_labels_deid.csv', index_col=0)\n",
    "train_parquet_scaled = pd.read_parquet('model_data/train_brain_data_deid_scaled.parquet')\n",
    "\n",
    "test_csv = pd.read_csv('model_data/test_brain_labels_deid.csv', index_col=0)\n",
    "test_parquet_scaled = pd.read_parquet('model_data/test_brain_data_deid_scaled.parquet')\n",
    "\n",
    "nd_csv = pd.read_csv('model_data/nd_brain_labels_deid.csv', index_col=0)\n",
    "nd_parquet_scaled = pd.read_parquet('model_data/nd_brain_data_deid_scaled.parquet')"
   ],
   "id": "7234b642f929a22c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load brain mask and MCALT background image",
   "id": "38b44194e58e9815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "brain_mask = nilearn.image.load_img('model_data/brain_mask.nii')\n",
    "bg = nilearn.image.load_img('model_data/t1.nii')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ],
   "id": "8301ccdf79aa8f47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example Visual of Standard Scaled FDG-PET",
   "id": "1418837c1e152540"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "img_data = np.zeros(brain_mask.shape)\n",
    "vec = nd_parquet_scaled.iloc[0, 0]\n",
    "nz_indices = np.ma.nonzero(brain_mask.get_fdata())\n",
    "img_data[nz_indices] = vec\n",
    "img = nilearn.image.new_img_like(brain_mask, img_data)\n",
    "nilearn.plotting.plot_img(img, cut_coords=(0, 0, 0))"
   ],
   "id": "dc1e85ac16744248"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Adjacency Matrix Initialization",
   "id": "658d193f5de829f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def k_matrix(coords, radius):\n",
    "    nn_search = NearestNeighbors(radius=radius)\n",
    "    nn_search.fit(coords)\n",
    "    \n",
    "    k = nn_search.radius_neighbors_graph(coords)\n",
    "    k = k.toarray()\n",
    "    return k.T"
   ],
   "id": "f5c7e82ed5246cc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_rows = nz_indices[0].size\n",
    "n_cols = 3\n",
    "arr = np.zeros((num_rows, n_cols))\n",
    "for d in range(3):\n",
    "    arr[:, d] = nz_indices[d]"
   ],
   "id": "f45d2436bb55c9f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 16\n",
    "radius = 2\n",
    "k = k_matrix(arr, radius)  #(16487, 16487), arr = (16487, 3) x, y, z of mask\n",
    "k = torch.tensor(k, dtype=torch.float32) + torch.eye(k.shape[0], dtype=torch.float32)\n",
    "degrees = torch.sum(k, axis=1)\n",
    "D = torch.diag(degrees)\n",
    "D_mod = torch.linalg.inv(torch.sqrt(D))\n",
    "k_normalized = torch.matmul(D_mod, torch.matmul(k, D_mod))\n",
    "k_normalized = k_normalized.to_sparse_csr()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ],
   "id": "264886771a390c04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Custom dataset",
   "id": "e3956e98cb43b697"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CustomLoader(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        super(CustomLoader, self).__init__()\n",
    "        \n",
    "        self.images = images # parquet\n",
    "        self.transform = transform\n",
    "\n",
    "        self.n_samples = self.images.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images.iloc[idx].copy()\n",
    "        image = np.array(image)\n",
    "        image = torch.from_numpy(image)\n",
    "        image = image.to(torch.float32)\n",
    "        image = image.unsqueeze(0)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ],
   "id": "544b492bab33c8a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load datasets using custom dataset loader",
   "id": "f8101c41157d114c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data = CustomLoader(images=train_parquet_scaled, transform=None)\n",
    "\n",
    "n_samples = train_data.__len__()\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "\n",
    "test_data = CustomLoader(images=test_parquet_scaled, transform=None)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "\n",
    "nd_data = CustomLoader(images=nd_parquet_scaled, transform=None)\n",
    "nd_loader = DataLoader(dataset=nd_data, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)"
   ],
   "id": "3fc183077cfc8ea4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Graph convolution method",
   "id": "afe19bebbfa1ff6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 264,
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, k):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.d1 = nn.Conv1d(in_dim, out_dim, kernel_size=1)\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        device = imgs.device\n",
    "        x = self.d1(imgs)\n",
    "        x = x.cpu() @ self.k.cpu()\n",
    "        x = x.to(device)\n",
    "        return x"
   ],
   "id": "2622949bfd4bdcc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Graph convolutional network block",
   "id": "aefdfef5a4973009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 470,
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, k):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.k = k\n",
    "        self.GC1 = GraphConv(in_dim, out_dim, self.k)\n",
    "        self.GC2 = GraphConv(out_dim, out_dim, self.k)\n",
    "        self.bn1 = nn.BatchNorm1d(out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "    def forward(self, x):\n",
    "        identity = x.mean(axis=1, keepdim = True)\n",
    "        out = self.GC1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.tanh(out) #out = F.relu(out)\n",
    "        out = self.GC2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + identity\n",
    "        #out = F.relu(out)  #testing\n",
    "        return out"
   ],
   "id": "a475b015343a83c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Encoder of the Variational autoencoder",
   "id": "1123edc690b3356a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 471,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, conv_dim, lin_dim, latent_dims, k, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.k = k\n",
    "        self.device = device\n",
    "        self.GCBlock = GCNBlock(in_dim, conv_dim, self.k)\n",
    "\n",
    "        self.Lin1 = nn.Linear(conv_dim * self.k.shape[0], lin_dim)\n",
    "        self.Lin2 = nn.Linear(lin_dim, latent_dims)\n",
    "        self.Lin3 = nn.Linear(lin_dim, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        #self.N.loc = self.N.loc.to(device)\n",
    "        #self.N.scale = self.N.scale.to(device)\n",
    "        self.KL = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.GCBlock(x)\n",
    "        x = F.tanh(x) #x = F.relu(x)\n",
    "        \n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.Lin1(x)\n",
    "        x = F.tanh(x) #x = F.relu(x)\n",
    "\n",
    "        mu = self.Lin2(x)\n",
    "        sigma = torch.exp(self.Lin3(x))\n",
    "        z = mu + sigma * self.N.sample(mu.shape).to(self.device)\n",
    "        self.KL = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "\n",
    "        return z"
   ],
   "id": "4f8d5ef2572efbe4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Decoder of the variational autoencoder",
   "id": "15fb9ad3211ac968"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 472,
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_dim, conv_dim, lin_dim,  latent_dims, k):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, lin_dim),\n",
    "            nn.Tanh(), #nn.relu(True),\n",
    "            nn.Linear(lin_dim, conv_dim * self.k.shape[0]), \n",
    "            nn.Tanh() #nn.relu(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(conv_dim, self.k.shape[0]))\n",
    "\n",
    "        self.decoder_conv = GCNBlock(conv_dim, in_dim, self.k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        \n",
    "        x = self.decoder_conv(x)\n",
    "        return x"
   ],
   "id": "e1a94546d89b9cb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Variational Autoencoder class",
   "id": "36263833507a6183"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 473,
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_dim, conv_dim, lin_dim, latent_dims, k, device):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = device\n",
    "        self.k = k\n",
    "        self.encoder = Encoder(in_dim, conv_dim, lin_dim, latent_dims, self.k, self.device)\n",
    "        self.decoder = Decoder(in_dim, conv_dim, lin_dim, latent_dims, self.k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ],
   "id": "8f60a286bff52b31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Optimizer and variational autoencoder initialization and hyperparameters",
   "id": "4cff7b2127328e13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 474,
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "latent_dims = 16\n",
    "vae = VAE(1, 8, 256, latent_dims=latent_dims, k=k_normalized, device=device)\n",
    "\n",
    "vae.to(device)\n",
    "total_epochs = 0"
   ],
   "id": "d32621627a8ac48c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 475,
   "source": [
    "lr = 1e-5\n",
    "optim = torch.optim.Adam(vae.parameters(), lr=lr)"
   ],
   "id": "48f4d35533ab00ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Pre-trained model accessible here:\n",
    "vae.load_state_dict(torch.load('model_data/2gc_16dim_model_scaled.pt'))\n",
    "\n",
    "optim.load_state_dict(torch.load('model_data/2gc_16dim_optim_scaled.pt'))"
   ],
   "id": "11023222c05c3f75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training and validation functions",
   "id": "e848712df1069ede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_epoch(vae, device, dataloader, optimizer):\n",
    "    vae.train()\n",
    "    train_loss = 0.0\n",
    "    perc = 0.75\n",
    "    for x in dataloader:\n",
    "        x = x.to(device)\n",
    "        x_hat = vae(x)\n",
    "        rec_loss = ((x - x_hat)**2).sum()\n",
    "        KL_loss = vae.encoder.KL\n",
    "        loss = (perc * rec_loss) + ((1 - perc) * KL_loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    return train_loss / len(dataloader.dataset)"
   ],
   "id": "7d49894bc3a3a6b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_epoch(vae, device, dataloader):\n",
    "    vae.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x in dataloader:\n",
    "            x = x.to(device)\n",
    "            x_hat = vae(x)\n",
    "            loss = ((x - x_hat)**2).sum()\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    return val_loss / len(dataloader.dataset)"
   ],
   "id": "394f567ff3e25a51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tloss_vals = []\n",
    "vloss_vals = []\n",
    "img_recon_vectors = []\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "org_test_img = batch[0, :].reshape(-1)"
   ],
   "id": "42a0bc02b5c359ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Epoch loop for training and testing",
   "id": "3e273f85a899f084"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_epochs = 50\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_epochs += 1\n",
    "    train_loss = train_epoch(vae, device, train_loader, optim)\n",
    "    val_loss = test_epoch(vae, device, test_loader)\n",
    "\n",
    "    tloss_vals.append(train_loss)\n",
    "    vloss_vals.append(val_loss)\n",
    "\n",
    "    rec_test_img = vae(batch)[0, :].reshape(-1).detach().numpy()\n",
    "    img_recon_vectors.append(rec_test_img.copy())\n",
    "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))"
   ],
   "id": "bd0755b1be1badb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Code to save model and optim parameters\n",
    "best_model_state = copy.deepcopy(vae.state_dict())\n",
    "\n",
    "torch.save(best_model_state, 'model_data/2gc_16dim_model_scaled.pt')\n",
    "\n",
    "best_optim_state = copy.deepcopy(optim.state_dict())\n",
    "\n",
    "torch.save(best_optim_state, 'model_data/2gc_16dim_optim_scaled.pt')"
   ],
   "id": "48c1b155cd70004e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss curves",
   "id": "f9a4742c6cd4b5db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_curves = pd.DataFrame(tloss_vals, vloss_vals)\n",
    "histogram_vals = pd.DataFrame((org_test_img, rec_test_img))"
   ],
   "id": "d1e26152026e7c68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def moving_avg(array, window_len=5):\n",
    "    avg_loss = np.zeros(len(array) - window_len)\n",
    "    for i in range(avg_loss.size):\n",
    "        avg_loss[i] = np.mean(array[i:i + window_len])\n",
    "    return avg_loss"
   ],
   "id": "2d624a2a00f4f108"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vloss_avg = moving_avg(vloss_vals, window_len=10)\n",
    "moving_avg = pd.DataFrame(vloss_avg)"
   ],
   "id": "5a1d577d4cbbc8dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
